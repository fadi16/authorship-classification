##########################################
Data
##########################################
Our dataset is collection of blog articles from 2004 and before, extracted from blogger.com
The datasets are huge, they're avaliable here: https://drive.google.com/drive/folders/1ObtlRTiRj8grnNlWKm4pAND1s1ib2tLU?usp=sharing
The link includes: blogtext.csv which is the original data set of 681285 blogs from more than 19000 authors - this was obtained from
(https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus)
The link also includes 2 directories: 10_authors and 50_authors containing datasets extracted from the original corpus for the 10 and 50 authors
with the largest number of texts.

Each directory includes training, validation and testing files created from the datasets we extracted for 10/50 authors, the split is as follows:
- 70% training
- 10% validation, this is used for early stopping and to prevent the model from overfitting
- 20% for testing

The files containing the keyword "balanced" have an equal number of texts for each author.
The files containing the keyword "pairs" are datasets of positive and negative pairs of text (positive means coming from the same author,
and negative means coming from a different author). These were created from the original files (the ones without the keyword "pairs") following a similar approach
to that proposed by Saedi and Dras (2021) - (page 9 from https://arxiv.org/pdf/1912.10616.pdf).
In short, we do the following (for each author):
- split the texts produced by each author into 4 equal sized chunks
- merge the first two chunks to create positive/same-author pairs for the given author
- split the third chunk into N - 1 pieces, where N is the total number of authors in the dataset
- merge the pieces from the third chunk with random texts from the other (N-1) authors' forth chunks
This results in an equal number of positive and negative pairs in these datasets.

The original dataset is not balanced, that is, there are some authors with more texts than other. We follow Fabien et al. (2020)
(https://aclanthology.org/2020.icon-main.16.pdf), which is the baseline we're following, in using a stratified approach,
meaning that the proportions of each class are kept equal in the training and testing set.
We also use the Class-Balanced Loss described by Cui et al. (2019) (https://openaccess.thecvf.com/content_CVPR_2019/papers/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.pdf)
as a way to deal with the balance issue.

We follow most/all previous AA works in not applying any pre-processing to the data (e.g. removing punctuation, casefolding, normalization, etc)
since these things encode important stylistic features for the model to learn. The only thing we really do here is just stripping
white spaces from the beginning and the end of the texts.


########################################
Solution
#######################################
All checkpoints available from https://drive.google.com/drive/folders/1g15L_eWjLjNN4CMrBhCnPTPCz9ysELtY?usp=sharing

We experimented with the following architectures (more details in their respective files)
    - Bert with classification head (classifier.py): current SoTA for blogs dataset (https://aclanthology.org/2020.icon-main.16/)
    - Bert Bi-Encoder (bi_encoder.py): a similarity based method for classification. Given a database of texts and their authors,
                  learn bi-encoder that can generate meaningful embeddings yield higher similarity for embeddings of
                  texts from same authors and lower similarity for texts from different authors.
                  Given a test sample:
                    - get the embedding of the given test sample,
                    - find its K most similar samples in the database
                    - attribute it to the author with the highest number of samples among those K.
    - Bert Bi-Encoder + Bert Cross Encoder (cross_encoder.py): use a cross encoder to further filter the K most similar samples in the database
                                  to the given test sample. Note that both cross and bi-encoder are trained seperately.

We used pytorch and HugginFace for the classifier and sentence-transformers for the Bi and cross encoders.
All models were finedtuned from the "bert-base-cased" checkpoint.
####################################
Evaluation
###################################
We run the model on the test set and get a csv file containing the precited and actual labels for each sample.
Then we use this csv file to evaluate the model (evaluation.py) on the following metrics:
    - accuracy
    - overall precision, recall, F1
    - per-class precision, recall F1
    - Matthew's Correlation Coefficient
    - Cohen's Kappa
    - Confusion Matrix
    - t-SNE: we plot the embeddings and manually inspect them to see if they
             are "somehow" clustered
